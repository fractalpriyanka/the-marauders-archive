# ğŸ“š RAG Storybook â€” The Marauderâ€™s Archive

A RAG-based question-answering system that lets users explore the entire Harry Potter collection through natural-language queries, returning answers grounded in the original text rather than speculative outputs.

The system loads PDFs, chunks and embeds content, indexes it with FAISS, retrieves relevant passages, and generates citation-supported responses using Google Gemini.

---

#### ğŸ“¸ Demo

![Demo](assets/screenrec.gif)

ğŸ‘‰ **Try the app here:**
[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://your-streamlit-app-link)

---

### ğŸš€ Features

ğŸ“„ Load very large PDFs (multi-book collections)

ğŸ” Smart sentence + chapter-aware chunking

ğŸ§  High-quality embeddings (all-mpnet-base-v2)

âš¡ Fast semantic search using FAISS

ğŸ¤– Answer generation using Google-GenAI

ğŸ§ª Retrieval evaluation (Recall@K)

ğŸŒ Streamlit interface (ready for deployment)

ğŸ“ Persistent pipeline files (pages.json, chunks.json, index.faiss)

---

### ğŸ—‚ï¸ Project Structure

```text
rag_storybook/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ generation/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ evaluation/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

#### âš™ï¸ 1ï¸âƒ£ Installation

**Create and activate a virtual environment:**

- python -m venv my_venv
- source my_venv/bin/activate # Windows: my_venv\Scripts\activate

**Install dependencies:**

- pip install -r requirements.txt

#### ğŸ”‘ 2ï¸âƒ£ Environment Variables

**Create a .env file in the project root:**

- GEMINI_API_KEY=your_key_here

#### ğŸ§¾ 3ï¸âƒ£ Pipeline â€” Step by Step

**3.1 Load PDF â†’ pages.json**

- python -m src.ingestion.pdf_loader --> data/processed/pages.json

```json
{
  "page_no": 2,
  "book": "Harry Potter and the Sorcerer's Stone",
  "chapter": "CHAPTER ONE: The Boy Who Lived",
  "text": "The Dursleys had everything they wanted..."
}
```

**3.2 Chunk Pages â†’ chunks.json**

- python -m src.chunking.chunker --> data/processed/chunks.json

```json
{
  "chunk_id": 6,
  "text": "couldnâ€™t help....",
  "book": "Harry Potter and the Sorcererâ€™s Stone",
  "chapter": "CHAPTER ONE: THE BOY WHO LIVED",
  "chapter_id": 0,
  "page_no": 12
}
```

**3.3 Generate Embeddings â†’ Build Index**

- python -m src.embeddings.embedder --> data/processed/index.faiss
- Uses: Sentence-Transformers â€” all-mpnet-base-v2

**3.4 Retrieval (FAISS + Context Expansion)**

When a user asks a question:

1ï¸âƒ£ encode query
2ï¸âƒ£ search FAISS
3ï¸âƒ£ expand neighboring chunks (chapter-aware)
4ï¸âƒ£ return ranked results

**3.5 Answer Generation (Google-GenAI)**

Retrieval text â†’ LLM prompt â†’ grounded answer.

No hallucinations â€” answer must come from retrieved context.

#### ğŸ§ª 4ï¸âƒ£ Evaluation

**Run evaluation:**

- python -m evaluation.evaluate_retrieval
- Outputs metrics: Recall@K -- Found / Missing questions

#### ğŸŒ 5ï¸âƒ£ Run the App (Streamlit)

- streamlit run app/main.py

### ARCHITECTURE

```text
PDF
 â”‚
 â–¼
Extract Pages  â€” (pdfplumber + regex)
 â”‚
 â–¼
Chunk Pages â€” (chapter-aware + sliding window)
 â”‚
 â–¼
Embeddings â€” (all-mpnet-base-v2)
 â”‚
 â–¼
FAISS Index â€” (IndexFlatIP, cosine)
 â”‚
 â–¼
User Question
 â”‚
 â–¼
Retrieve â€” (FAISS + neighbors + lexical boost)
 â”‚
 â–¼
Gemini Answer â€” (grounded, no hallucinations)

```

## Harry Potter RAG System â€“ Evaluation Report

### Project Overview

This document summarizes the evaluation of a Retrieval-Augmented Generation (RAG) system built on the complete Harry Potter book series (Books 1â€“7).
The evaluation focuses on retrieval quality, grounding, and reliability, not memorization or trivia.

**System**: Harry Potter RAG
**Source Material**: Harry Potter Complete Series (Books 1â€“7)
**Evaluation Dataset**: 20 curated book-only questions

### Evaluation Methodology

#### Test Dataset Composition

The system was tested across multiple question paradigms to reflect real narrative QA behavior:

```text

| Paradigm               | Count | Description                           |
| ---------------------- | ----- | ------------------------------------- |
| **Explicit Facts**     | 5     | Direct, clearly stated facts          |
| **Rare Facts**         | 4     | Details mentioned once or formally    |
| **Emergent Narrative** | 4     | Facts requiring cross-chapter context |
| **Locations**          | 4     | Spatial and setting questions         |
| **Poems / Songs**      | 2     | Structured verse content              |
| **Boundary Cases**     | 1     | Out-of-scope (should refuse)          |
```

#### Key Results

#### Overall Metrics

```text
| Metric                     | Score    | Status      |
| -------------------------- | -------- | ----------- |
| **Retrieval Recall@K**     | **73%**  | âœ… Decent    |
| **Explicit Fact Accuracy** | High     | âœ… Strong    |
| **Rare / Emergent Facts**  | Moderate | âš ï¸ Expected |
| **Hallucination Rate**     | Low      | âœ… Safe      |
| **Correct Refusals**       | 100%     | âœ… Correct   |
```

#### Performance by Paradigm

```text

| Question Type      | Correct / Total | Notes                        |
| ------------------ | --------------- | ---------------------------- |
| Explicit Facts     | 5 / 5           | Reliable retrieval           |
| Rare Facts         | 2 / 4           | Single-mention limitation    |
| Emergent Narrative | 2 / 4           | Multi-hop reasoning required |
| Locations          | 3 / 4           | Phrasing sensitivity         |
| Poems / Songs      | 2 / 2           | Chunking effective           |
| Boundary Case      | 1 / 1           | Proper refusal               |
```

#### Detailed Analysis

##### Strengths

âœ… Strong performance on explicit facts and object/entity queries
âœ… Successful retrieval of songs and poems
âœ… Low hallucination rate due to strict grounding
âœ… Correct handling of out-of-scope questions

##### Weaknesses

âŒ Rare facts mentioned only once (e.g., full formal names)
âŒ Emergent facts requiring synthesis across distant chapters
âŒ Location questions with varied phrasing

These failures are expected for narrative RAG systems and do not indicate architectural flaws.

#### Interpretation & Insights

**What This Evaluation Measures**

âœ… Retrieval quality (Recall@K)
âœ… Grounded answer generation
âœ… Faithfulness to source material
âœ… Refusal behavior for missing context

### Conclusion

With a Retrieval Recall@K of 0.73, the Harry Potter RAG system demonstrates solid and realistic performance for a long, narrative, multi-book corpus.

**Key takeaway:**

The system prioritizes faithfulness over fluency, correctly refusing uncertain answers rather than hallucinating â€” the desired behavior for a trustworthy RAG system.

### ğŸ“Œ Configuration â€” settings.py

**Central control over:**

- paths

- chunking

- embedding model

- FAISS parameters

- Gemini model

- TOP-K retrieval

### ğŸ”§ Improving Retrieval (Optional Enhancements)

- tune chunk size + overlap

- add title/heading boosting

- try hybrid (keyword + vector) search

- filter irrelevant passages before LLM

- increase context window slightly

### ğŸ¤ Credits

**Built with:**

- Sentence-Transformers

- FAISS

- Google-GenAI

- Streamlit

- Python

Inspired by classic RAG architecture â€” adapted for large storybooks.

#### ğŸ“„ License

Educational / personal use only.
Content belongs to original copyright holders.
